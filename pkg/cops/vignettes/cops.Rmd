---
title: "A tutorial on Cluster Optimized Proximity Scaling (COPS)"
author: "Thomas Rusch"
date: "`r Sys.Date()"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this document we give a high-level, relatively non technical introduction to the functionality available in the `cops` package for fitting multidimensional scaling (MDS; Borg & Groenen 2005) models that have an emphasis on providing a clustered configuration. We start with a short introduction to COPS and the models that we have available. We then explain fitting of these models with the `cops` package and show how to fit those. For illustration we use the `smacof::kinshipdelta` data set (Rosenberg, S. & Kim, M. P., 1975) which lists percentages of how often 15 kinship terms were not grouped together by college students. 

```{r}
library(cops)
```

## Proximity Scaling
For proximity scaling (PS) or multidimensional scaling (MDS) the input is typically an $N\times N$ matrix $\Delta^*=f(\Delta)$, a matrix of proximities with elements $\delta^*_{ij}$, that is a function of a matrix of observed non-negative dissimilarities $\Delta$ with elements $\delta_{ij}$. $\Delta^*$ usually is symmetric (but does not need to be). The main diagonal of $\Delta$ is 0. We call a $f: \delta_{ij} \mapsto \delta^*_{ij}$ a proximity transformation function. In the MDS literature these $\delta_{ij}^*$ are often called dhats or disparities. The problem that proximity scaling solves is to locate an $N \times M$ matrix $X$ (the configuration) with row vectors $x_i, i=1,\ldots,N$ in low-dimensional space $(\mathbb{R}^M, M \leq N)$ in such a way that transformations $g(d_{ij}(X))$ of the fitted distances $d_{ij}(X)=d(x_i,x_j)$---i.e., the distance between different $x_i, x_j$---approximate the $\delta^*_{ij}$ as closely as possible. We call a $g: d_{ij}(X) \mapsto d_{ij}^*(X)$ a distance transformation function. In other words, proximity scaling means finding $X$ so that $d^*_{ij}(X)=g(d_{ij}(X))\approx\delta^*_{ij}=f(\delta_{ij})$. 

This approximation $D^*(X)$ to the matrix $\Delta^*$ is found by defining a badness-of-fit criterion (loss function), $\sigma_{MDS}(X)=L(\Delta^*,D^*(X);\Gamma(X))$, that is used to measure how closely $D^*(X)$ approximates $\Delta^*$, optionally subject to an additional criterion of the appearance of $X$, $\Gamma(X)$. The smaller the badness-of-fit, the better the fit is. 

The loss function used is then minimized to find the vectors $x_1,\dots,x_N$, i.e., 
\begin{equation}
\label{eq:optim}
\arg \min_{X}\ \sigma_{MDS}(X).
\end{equation}
There are a number of optimization techniques one can use to solve this optimization problem. 


### Stress Models 
Usually, we use the quadratic loss function. A general formulation of a loss function based on a quadratic loss is known as _stress_ (Kruskall 1964) and is 
\begin{equation}
 \label{eq:stress}
 \sigma_{MDS}(X)=\sum^N_{i=1}\sum^N_{j=1} z_{ij} w_{ij}\left[d^*_{ij}(X)-\delta^*_{ij}\right]^2=\sum^N_{i=1}\sum^N_{j=1} z_{ij}w_{ij}\left[g\left(d_{ij}(X)\right)-f(\delta_{ij})\right]^2
\end{equation}
where we use some type of Minkowski distance ($p > 0$) as the distance fitted to the points in the configuration, 
\begin{equation}
\label{eq:dist}
d_{ij}(X) = ||x_{i}-x_{j}||_p=\left( \sum_{m=1}^M |x_{im}-x_{jm}|^p \right)^{1/p} \ i,j = 1, \dots, N.
\end{equation}
Typically, the norm used is the Euclidean norm, so $p=2$. In standard MDS $g(\cdot)=f(\cdot)=I(\cdot)$, the identity function. The $w_{ij}$ and $z_{ij}$ are finite weights, e.g., with $z_{ij}=0$ if the entry is missing and $z_{ij}=1$ otherwise.  

This formulation enables one to express a large number of popular MDS methods with `cops`. Generally, we allow to use specific choices for $f(\cdot)$ and $g(\cdot)$ from the family of power transformations so one can fit the following stress models:  

* **Explicitly normalized stress**: $w_{ij}=(\sum_{ij}\delta^{*2}_{ij})^{-1}$, $\delta_{ij}^*=\delta_{ij}$, $d_{ij}(X)^*=d_{ij}(X)$
* **Stress-1**: $w_{ij}=(\sum_{ij} d^{*2}_{ij}(X))^{-1}$, $\delta_{ij}^*=\delta_{ij}$, $d_{ij}(X)^*=d_{ij}(X)$ 
* **Sammon stress** (Sammon 1969): $w_{ij}=\delta^{*-1}_{ij}$ , $\delta_{ij}^*=\delta_{ij}$, $d_{ij}(X)^*=d_{ij}(X)$
* **Elastic scaling** stress (McGee 1966): $w_{ij}=\delta^{*-2}_{ij}$,  $\delta_{ij}^*=\delta_{ij}$, $d_{ij}(X)^*=d_{ij}(X)$
* **S-stress** (Takane et al. 1977): $\delta^*_{ij}=\delta_{ij}^2$ and $d^*_{ij}(X)=d^2_{ij}(X)$, $w_{ij}=1$
* **R-stress** (de Leeuw, 2014): $\delta^*_{ij}=\delta_{ij}$ and $d^*_{ij}=d^{2r}_{ij}$, $w_{ij}=1$  
* **Power MDS** (Buja et al. 2008, Rusch et al. 2015a): $\delta^*_{ij}=\delta_{ij}^\lambda$ and $d^*_{ij}=d_{ij}^\kappa$, $w_{ij}=1$
* **Power elastic scaling** (Buja et al. 2008, Rusch et al. 2015a): $w_{ij}=\delta^{*-2}_{ij}$, $\delta^*_{ij}=\delta_{ij}^\lambda$ and $d^*_{ij}=d^\kappa_{ij}$
* **Power Sammon mapping** (Buja et al. 2008, Rusch et al. 2015a): $w_{ij}=\delta^{*-1}_{ij}$, $\delta^*_{ij}=\delta_{ij}^\lambda$ and $d^*_{ij}=d_{ij}^\kappa$
* **Approximate Powerstress** (Rusch et al. 2020): $\delta^*_{ij}=\delta_{ij}^\lambda$ and $d^*_{ij}=d_{ij}$, $w_{ij}=delta_{ij}^\nu$.
* **Restricted Powerstress** (Buja et al. 2008, Rusch et al. 2015a): $\delta^*_{ij}=\delta_{ij}^\kappa$ and $d^*_{ij}=d^\kappa_{ij}$, $w_{ij}=w_{ij}^\nu$ for arbitrary $w_{ij}$ (e.g., a function of the $\delta_{ij}$)
* **Powerstress** (encompassing all previous models; Buja et al. 2008, Rusch et al. 2015a): $\delta^*_{ij}=\delta_{ij}^\lambda$, $d^*_{ij}=d_{ij}^\kappa$ and $w_{ij}=w_{ij}^\nu$ for arbitrary $w_{ij}$ (e.g., a function of the $\delta_{ij}$)

For all of these models one can use the function `powerStressMin` which uses majorization to find the solution (de Leeuw, 2014). The function allows to specify a `kappa`, `lambda` and `nu` argument as well as a `weightmat` (the $w_{ij}$), by setting the respective argument. For some models (those without transformations for the $d_{ij}$) one can use `smacof::mds`.
 
[!!]: Note that if $z_{ij}$ and $w_{ij}$ are used, then `weightmat` must be the combination of both.  

The object returned from a call to `powerStressMin` is of class `smacofP` which extends the `smacof` classes (de Leeuw & Mair, 2009) to allow for the power transformations. Apart from that the objects are made so that they have maximum compatibility to methods from `smacof`. Accordingly, the following S3 methods are available:

| Method      | Description                   |        
|:------------|:------------------------------|   
|print        | Prints the object             |
|summary      | A summary of the object       |
|plot         | 2D Plots of the object        |
|plot3d       | Dynamic 3D configuration plot |
|plot3dstatic | Static 3D configuration plot  |
|residuals    | Residuals                     |
|coef         | Model Coefficients            | 

Let us illustrate the usage

```{r}
dis<-as.matrix(smacof::kinshipdelta)
```  

* A standard MDS (`stress`)
```{r}
res1<-powerStressMin(dis,kappa=1,lambda=1)
res1
```

* A `sammon` mapping
```{r}
res2<-powerStressMin(dis,kappa=1,lambda=1,nu=-1,weightmat=dis)
res2
```

Alternatively, one can use the faster `sammon` function from `MASS` (Venables & Ripley, 2002) for which we provide a wrapper that adds class attributes and methods (and overloads the function). 

```{r}
res2a<-sammon(dis)
res2a
```


* An `elastic` scaling
```{r}
res3<-powerStressMin(dis,kappa=1,lambda=1,nu=-2,weightmat=dis)
res3
```

* A `sstress` model 
```{r}
res4<-powerStressMin(dis,kappa=2,lambda=2)
res4
```

* An `rstress` model (with $r=1$ as $r=\kappa/2$)
```{r}
res5<-powerStressMin(dis,kappa=2,lambda=1)
res5
```

* A `powermds` model 
```{r}
res6<-powerStressMin(dis,kappa=2,lambda=1.5)
res6
```

* A `powersammon` model 
```{r}
res7<-powerStressMin(dis,kappa=2,lambda=1.5,nu=-1,weightmat=dis)
res7
```

* A `powerelastic` scaling
```{r}
res8<-powerStressMin(dis,kappa=2,lambda=1.5,nu=-2,weightmat=dis)
res8
```
* A `restricted powerstress` model 
```{r}
res9<-powerStressMin(dis,kappa=1.5,lambda=1.5,nu=-1.5,weightmat=2*1-diag(nrow(dis)))
res9
summary(res9)
```

* A `powerstress` model 
```{r}
res10<-powerStressMin(dis,kappa=2,lambda=1.5,nu=-1.5,weightmat=2*1-diag(nrow(dis)))
res10
summary(res10)
```

```{r,eval=FALSE,fig.show='hold',fig.width=8, fig.height = 8}
plot(res9)
plot(res9,"transplot")
plot(res9,"Shepard")
plot(res9,"resplot")
plot(res9,"bubbleplot")
```
 
### Strain Models
Another popular type of MDS supported by `cops` is based on the loss function type \emph{strain}. Here the $\Delta^*$ are a transformation of the $\Delta$, $\Delta^*= f (\Delta)$ so that $f(\cdot)=-(h\circ l)(\cdot)$ where $l$ is any function and $h(\cdot)$ is a double centering operation, $h(\Delta)=\Delta-\Delta_{i.}-\Delta_{.j}+\Delta_{..}$ where $\Delta_{i.}, \Delta_{.j}, \Delta_{..}$ are matrices consisting of the row, column and grand marginal means respectively. These then get approximated by (functions of) the inner product matrices of $X$ 
\begin{equation}
\label{eq:dist2}
d_{ij}(X) = \langle x_{i},x_{j} \rangle
\end{equation}
We can thus express classical scaling as a special case of the general PS loss with $d_{ij}(X)$ as an inner product, $g(\cdot) = I(\cdot)$ and $f(\cdot)=-(h \circ I)(\cdot)$.

If we again allow power transformations for $g(\cdot)$ and $f(\cdot)$ one can fit the following strain models with `cops` 

* **Classical scaling** (Torgerson, 1958): $\delta^*_{ij}=-h(\delta_{ij})$ and $d^*_{ij}=d_{ij}$ 
* **Powerstrain** (Buja et al. 2008, Rusch et al. 2015a): $\delta^*_{ij}=-h(\delta_{ij}^\lambda)$, $d^*_{ij}=d_{ij}$ and $w_{ij}=w_{ij}^\nu$ for arbitrary $w_{ij}$

In `stops` we have a wrapper to `cmdscale` (overloading the `base` function) which extend functionality by offering an object that matches `smacofP` objects with corresponding methods.

Let us illustrate the usage. A `powerstrain` model is rather easy to fit with simply subjecting the dissimilarity matrix to some power. Here we use $\lambda=3$.

```{r}
resc<-cmdscale(kinshipdelta^3)
resc
```

```{r}
summary(resc)
```

```{r,eval=FALSE,fig.show='hold',fig.width=8, fig.height=8}
summary(resc)
plot(resc)
```

The models listed above are also available as dedicted wrapper functions with a `cop_` prefix

|Function                  | Model                       |        
|:-------------------------|:----------------------------|   
|`cop_cmdscale`            | Strain/Powerstrain          |
|`cop_smacofSym`           | Stress                      |
|`cop_smacofSphere`        | Smacof on a sphere          |
|`cop_sammon`,`cop_sammon2`| Sammon scaling              |
|`cop_elastic`             | Elastic scaling             |
|`cop_sstress`             | S-stress                    |
|`cop_rstress`             | r-stress                    |
|`cop_powermds`            | Powermds                    |
|`cop_powersammon`         | Sammon scaling with powers  |
|`cop_powerelastic`        | Elastic scaling with powers |
|`cop_apstress`            | Approximate power stress    | 
|`cop_powerstress`         | Powerstress                 |
|`cop_rpowerstress`        | Restricted Powerstress      | 


## Augmenting MDS with clusteredness considerations: COPS 

The main contribution of the `cops` package is not in solely fitting the _powerstress_ or _powerstrain_ models and their variants from above, but to augment the badness-of-fit function to achieve a "structured" MDS result automatically (in the sense of clusters or discrete structures). This can be useful mainly for exploring or generating discrete structures or to preserve clusters. 

For this an MDS loss function is augmented to include a penalty. This combination of an MDS loss with a clusteredness penalty is what we call "cluster optimized stress" (copstress) and the resulting MDS is coined "Cluster Optimized Proximity Scaling" (or COPS). This is a multi-objective optimization problem as we want to simultaneously minimize badness-of-fit *and* maximize clusteredness. The computational problem is solved by combining the two, but interpretation should happen individually with the badness-of-fit and clusteredness values respectively. 

We allow two ways of how copstress can be used: In one variant (COPS-C) one looks for an optimal configuration $X^*$ directly, given the transformation parameters. This yields a configuation that has a more clustered appearance than the standard MDS with the same tranformation parameters. In the other (P-COPS) we automatically select optimal transformation parameters and then solve the respective transformed MDS so that the clusteredness appearance of the configuation is improved. 

### COPS-C: Finding a configuration with COPS

Here we combine a normalized stress function $\sigma'_{\text{stress}}(X|\bm\theta)$ given stress hyperparameter vector $\bm\theta$ and a measure of clusteredness, the OPTICS cordillera ($\text{OC}'(X)$) to the following objective 
\begin{equation}
\label{eq:spstressv1}
\sigma_{\text{PS}}(X|\bm\theta)=\text{copstress}(X|\bm\theta) = v_1 \cdot \sigma'_{\text{stress}}(X|\bm\theta) - v_2 \cdot \text{OC}_\gamma'(X), 
\end{equation}
with scalarization weights $v_1,v_2 \in \mathbb{R}_+$, which is minimized over all possible $X$.

In COPS-C the parameters $\bm\theta, v_1, v_2$ and $\gamma$ are all treated as given. We recommend to use the convex combination $v_2=1-v_1$ with $0 \leq v_1 \leq 1$. For a given $\bm\theta$ if $v_2=0$ the result of (\ref{eq:spstressv1}) is the same as solving the respective stress problem, minimizing copstress in this variant jitters the configuration towards a more clustered arrangement, the strength of which is governed by the values of $v_1, v_2$. 

If the $\sigma'_{\text{stress}}(X|\bm\theta)$ allows for different transformation of dissimilarities and distances (e.g., powerstress), we expect researchers and practictioners to start from identic transformations. If need arises, e.g., to avoid a problem of near-indifferentiation, one can exploit the flexibility of employing different transformations. For that case we point out that the configuration may then represent a relation that is somewhat further apart of the main aim in MDS of faithfully reproducing the dissimilarities by distances in a comparable space but may allow some desired aspects to be revealed in a graphical representation.

COPS-C can be used either for improving c-clusteredness for a given initial MDS configuration (which may then be only locally optimal) or for looking for the globally near-optimal COPS-C configuration (with different starting configurations, see below).

#### Usage and Examples
COPS-C can be fit with the function `copstressMin`. The mandatory arguments is `delta` which is the dissimilarity matrix and COPS-C can be fit as

```{r}
cc.res1<-copstressMin(kinshipdelta)
cc.res1
```

The print function outputs information about the COPS-C model. In this case we fitted a ratio COPS-C that uses the standard MDS stress (all transformation parameters 1). There were 15 objects and the square root of stress of the configuration is 0.268 (compared to 0.267 for standard MDS, see above). The normed OPTICS cordillera value is 0.245, compared to 0.13 for standard MDS (with 0 being no clusteredness and 1 perfect clusteredness, see below). We also get information on $v_1$ and $V_2$ which were 0.975 and 0.025 respectively (the default values). The copstress value is 0.255, but we stress that this isn't particulalry important for interpretation.

The values that we should interpret are the stress and the cordillera. We see that the badness-of-fit for the COPS-C configuration is a bit higher (which is to be expected due to the penalization) and also that clusteredness increased by quite a bit. This is also evident in the Procrustes plot (grey is standard MDS, coral is COPS-C). 

```{r,echo=FALSE}
cc.res0<-copstressMin(kinshipdelta,stressweight=1,cordweight=0
cc.res0
```
```{r,fig=TRUE}
plot(Procrustes(cc.res0$conf,cc.res1$conf),legend=list(labels=c("Standard MDS","COPS-C")))
```
```{r,echo=FALSE}
par(mfrow=c(1,2))
plot(cc.res0,main="standard MDS",ylim=c(-2,2))
plot(cc.res1,main="COPS-C",ylim=c(-2,2))
```

Specifically, the clusters of "Sister, Daughter, Mother" and "Son, Brother, Father" as well as "Grandson, Grandfather" are a bit more compact for the COPS-C result as compared to the standard MDS, and "Cousin" has been moved slightl towards "Uncle, Nephew". At the same time, the fit is almost equal (0.268 for COPS-C vs. 267 for MDS).

We can also look at the clusteredness situation with the OPTICS reachability plot, which shows more clusteredness for COPS-C (a stronger up and down of the black line over the reachabilities). Next to the more compact clusters (deeper valleys) the main difference for COPS-C is that with the default minimum points that must form a cluster being 3 (default) in this case means that cousin is now also part of a three object cluster (with low density) and not a noise point as in standard MDS.

```{r,fig=TRUE}
par(mfrow=c(1,2))
plot(cc.res0,plot.type="reachplot",main="standard MDS")
plot(cc.res1,plot.type="reachplot",main="COPS-C")
```
In the above example, we used the default setup, but there are many ways to customize the COPS-C model.

The number of iterations can be changed with the `itmax` argument (defaults to 5000). If it is low, a warning is returned but that should usually be rather inconsequential. Let's set the iterations to 20000 (where the warning no longer appears but the copstress value is only a slightly lower). If one values accuracy over computation time, then a higher value is preferable. 

```{r}
cc.res1.20000<-copstressMin(kinshipdelta,itmax=20000)
cc.res1.20000
```

If we want to find the approximation in $R^N$ we can change the `ndim` argument, where $ndim=N$. Default is a 2D space, so `ndim=2. Let's do a COPS-C in a 3D target space.

```{r}
cc.res1a<-copstressMin(kinshipdelta,ndim=3)
cc.res1a$conf
```

An important parameter is the minimum number that must comprise a cluster, `minpts`. Default is `ndim+1`, which is typically 3 but should really be selected based on substantive considerations (and must be $\geq 2$). It can also be varied in different runs to explore the clusteredness structure. If we set minpts=2, we see that the 2 object clusters are pushed more towards each other

```{r}
cc.res2<-copstressMin(kinshipdelta,minpts=2)
plot(cc.res2)
```

The scalarization weights $v_1, v_2$ can be changed with `stressweight` and `cordweight`. They encode how strong stress and cordillera should respectively be weighted for the scalarization. The higher `stressweight` is in relation to `cordweight` the more weight is put on stress (so a more faithful representation to the MDS result). The default values are `stressweight=0.975` and `cordweight=0.025`. We suggest to put much more weight on stress to not create an articifical configuration. Let's look at the effect of changing it to `stressweight=0.8`, `cordweight=0.2`--- we see we have much more clusteredness now (0.73) but badness-of-fit has also ramped up a lot to 0.33 and the representation may no longer be very faithful to the real dissimilarities.

```{r}
cc.res3<-copstressMin(kinshipdelta,stressweight=0.8,cordweight=0.2)
cc.res3
```
```{r,fig=TRUE}
plot(cc.res3)
```

The arguments `kappa`,`lambda`,`nu` and `theta` all allow to fit power transformations (if a `theta` is given it overrides the other values), with `kappa` being the distance power transformation, `lambda` the proximity power transformation and `nu` the power transformation for the weights. `theta` is a vector collecting $c(kappa,lambda,nu)$. Let's fit an s-stress COPS-C.

```{r}
cc.res4<-copstressMin(kinshipdelta,kappa=2,lambda=2)
cc.res4
cc.res4a<-copstressMin(kinshipdelta,theta=c(2,2,1))
cc.res4a
```

So far we fit COPS-C with ratio MDS and power transformations. It is also possible to fit interval and ordinal (non-metric MDS) with the `type` argument. For non-metric MDS there also is a choice of how to handle `ties`, with the options of "primary", "secondary" or "tertiary". See `?smacof::mds` for more. Note that it is currently not possible to use transformation parameters with interval and non-metric MDS. Let's fit a non-metric COPS-C model.

```{r}
cc.res5<-copstressMin(kinshipdelta,type="ordinal")
cc.res5
```

Because COPS-C uses the OPTICS cordillera to measure clusteredness, it is possible to change a few parameters of how clusteredness is measured. `minpts` is the most important one and we already discussed that. Additional parameters include `q` which is the parameter for the $L_p$-norm of the OPTICS Cordillera and is typically 1 (default) or 2. A higher value of `q` can be thought of as pronouncing the ups and downs relatively stronger. The parameter `epsilon` relates to the maximum neighbourhood radius around a point to look for possible other points in a cluster and also relates to the density that a cluster must have. It influences the number of points that are classified as noise by OPTICS and improves runtime of OPTICS the smaller it is. It is not a praticularly intuitive parameter but for most MDS application it should suffice to just set it "sufficiently large" so all points are considered as possible neighbours of each other. It should only be changed to a lower value if the concept of "noise points" is useful for a data set (e.g., objects that are not supposed to be in a cluster anyway). Finally `dmax` and `rang` relate to the normalization and winsorization distance for the cordillera, essentially as the maximum distance between points that we still take into account. This can be used for make the index more robust to outliers in the configuration so that the algorithm doesn't just achieve a higher index by placing some points very far away from the rest. If `dmax` is NULL, the normalization is set to (0,1.5 x the maximum reachability distance for the torgerson model). If it is set too low, the normed cordillera value may be too high. Similarly, `rang` alows to set the whole normalization interval and is (0,dmax) by default. If `max(rang)` and `dmax` do not agree a warning is printed and `rang` takes precedence. These parameters can be used to explor different winsorization limits for robustness checks.

Let's look at their effects. First we set `q=2` and see that the effect of clusteredness is a bit more pronounced as compared to `q=1` (because larger ups and downs in the cordillera are weighted a higher)

```{r}
cc.res6<-copstressMin(kinshipdelta,q=2)
plot(cc.res6,plot.type="reachplot") 
```

Let's lower `epsilon` to 0.6 and `minpts` to 2 which means that points that are beyond that distance are no longer possible to be considered as cluster members of each other which allows COPS to have "Sister" and "Brother" pushed out of their respective clusters of "Daughter, Mother" and "Father, Son" and have all those two object clusters really tightly packed. The single objects would now be noise points.

```{r}
cc.res6a<-copstressMin(kinshipdelta,epsilon=0.6,minpts=2)
plot(cc.res6a,plot.type="reachplot")
plot(cc.res6a)
```
Let's also change `dmax` to 1 to make the index more robust. The effect is that "Cousin" is now less far away from the rest.

```{r}
cc.res6b<-copstressMin(kinshipdelta,dmax=1)
plot(cc.res6b)
```

Finally, we have `scale` which influences the scale of the axis. In COPS we're only interested in the relatvie placement of the objects rather than the scale, so the scale is somewhat aribtrary. It can be set to be `sd` (divided by the largest standard deviation of any column; default), `none` where no scaling is applied,  `proc` which deos procrustes adjustment of the final configuration to the starting configuration, `rmsq` (configuration divided by the maximum root mean square of the columns) and `std` which standardizes all columns (NOTE: this does not preserve the relative distances of the optimal configuration and should probably never have been implemented in the first place).

There are some more arguments which are described in `?copstressMin`.

COPS-C is a very difficult optimization problem and we resort to heuristics to solve it. There are a large number of such global optimization heuristics supported in `cops`. The default is `hjk-Newuoa` and will typically work quite well. Another good optimizer is `CMA-ES` but that has a tendency to fail. See `?copstressMin` for the available solvers for the argument `optimmethod`. 


### P-COPS


A special STOPS model is COPS (Rusch et al. 2015a) for "Cluster Optimized Proximity Scaling". This is also one of the main use cases for STOPS models. Let us write $X(\theta)=\arg\min_X \sigma_{MDS}(X,\theta)$ for the optimal configuration for given transformation parameter $\theta$. Following the outline of STOPS the overall objective function, which we call \emph{cluster optimized loss (coploss)}, is a weighted combination of the $\theta-$parametrized loss function, $\sigma_{MDS}\left(X(\theta),\theta\right)$, and a c-clusteredness measure, the OPTICS cordillera or $OC(X(\theta);\epsilon,k,q)$ to be optimized as a function of $\theta$ or
\begin{equation}
\label{eq:spstress}
\text{coploss}(\theta) = v_1 \cdot \sigma_{MDS}\left(X(\theta),\theta \right) - v_2 \cdot \text{OC}\left(X(\theta);\epsilon,k,q\right) 
\end{equation}
with $v_1,v_2 \in \mathbb{R}$ controlling how much weight should be given to the scaling fit measure and the c-clusteredness. In general $v_2,v_2$ are either \emph{a priori} determined values that make sense for the application or may be used to trade-off fit and c-clusteredness in a way for them to be commensurable. In the latter case we suggest taking the fit function value as it is ($v_1=1$) and fixing the scale such that $\text{coploss}=0$ for the scaling result with no transformations ($\theta=\theta_0$), i.e.,
\begin{equation}
\label{eq:spconstant0}
v^{0}_{1}=1, \quad v^{0}_2=\frac{\sigma_{MDS}\left(X(\theta_0),\theta_0\right)}{\text{OC}\left(X(\theta_0);\epsilon,k,q\right)},
\end{equation}
with $\theta_0=(1,1)^\top$ in case of loss functions with power transformations. Thus an increase of 1 in the MDS loss measure can be compensated by an increase of $v^0_1/v^0_2$ in c-clusteredness. Selecting $v_1=1,v_2=v^{0}_2$ this way is in line with the idea of pushing the configurations towards a more clustered appearance relative to the initial solution. 

 Another possibility is to choose them in such a way that $\text{coploss}=0$ in the optimum value, i.e., choosing $v^{opt}_{1}, v^{opt}_2$ so that 
  \begin{equation}
v^{opt}_1 \cdot \sigma_{MDS}\left(X(\theta^*),\theta^*\right)-v^{opt}_2 \cdot \text{OC}\left(X(\theta^*);\epsilon,k,q \right) = 0 
\end{equation}
with $\theta^*:=\arg\min_\theta \text{coploss}(\theta)$. This is in line with having $\text{coploss}(\theta)>0$ for $\theta \neq \theta^*$ and allows to optimize over $v_1,v_2$. 

The optimization problem in COPS is then to find    
\begin{equation}
\label{eq:soemdsopt2}
\arg\min_{\theta} \text{coploss}(\theta)
\end{equation}
by doing
\begin{equation} 
\label{eq:soemdsopt}
v_1 \cdot \sigma_{MDS}\left(X(\theta),\theta\right) - v_2 \cdot \text{OC}\left(X(\theta);\epsilon,k,q\right) \rightarrow \min_\theta! 
\end{equation}
For a given $\theta$ if $v_2=0$ than the result of optimizing the above is the same as solving the respective original PS problem. Letting $\theta$ be variable, $v_2=0$ will minimize the loss over configurations obtained from using different $\theta$.

<!-- [//]: Minimizing coploss is difficult. In `stops` we use a nested algorithm combining optimization that internally first solves for $X$ given $\theta$, $\arg\min_X \sigma_{MDS}\left(X,\theta\right)$, and then optimize over $\theta$ with a metaheuristic. Implemented are a simulated annealing (`optimmethod="SANN"`) or particle swarm optimization (`optimmethod="pso"`) and a variant of the Luus-Jaakola (`optimmethod="ALJ"`) procedure \citep{luus1973optimization} to be used in Step 3 that usually converges in less than 200 iterations to an acceptable solution. We suggest to use the latter.
--!>

The c-clusteredness index we use is the OPTICS cordillera which measures how clustered a configuration appears. It is based on the OPTICS algorithm that outputs an ordering together with a distance. The OPTICS cordillera is now simply an agregation of that information. Since we know what constitutes a maximally clustered result, we can derive an upper bound and normalize the index to lie between 0 and 1. If it is maximally clustered, the index gets a value of 1,and it gets 0 if all points are equidistant to their nearest neighbours (a matchstick embedding). See Rusch et al (2015a) for details. 

#### Usage 
Even though one can fit a COPS model with `stops`, there is a dedicated function `cops`. Its syntax works pretty much like in `stops` only that the `structure` argument is non-existant. 

`cops(dis,loss,...)`

For the example we use a COPS model for a classical scaling (`strain` loss) 

```{r}
resc<-cops(kinshipdelta,loss="strain")
resc
summary(resc)
```
A number of plots are availabe

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(resc,"confplot")
plot(resc,"Shepard")
plot(resc,"transplot")
plot(resc,"reachplot")
````

For convenience it is also possible to use the `cops` function for finding the loss-optimal transformation in the the non-augmented models specified in `loss`, by setting the `cordweight`, the weight of the OPTICS cordillera, to 0. Then the function optimizes the MDS loss function only. 

```{r}
resca<-cops(kinshipdelta,cordweight=0,loss="strain")
resca
```

Here the results match the result from using the standard `cordweight`. We can give more weight to the c-clusteredness though:

```{r}
rescb<-cops(kinshipdelta,cordweight=20,loss="strain")
rescb
```

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(resca,main="with cordweight=0")
plot(rescb,main="with cordweight=20")
```

This result has more c-clusteredness but less fit. The higher c-clusteredness is discernable in the Grandfather/Brother and Grandmother/Sister clusters (we used a minimum number of 2 observations to make up a cluster, `minpts=2`). 


### STOPS

Following Rusch et al. (2015b) the general idea  is that from given observations $\Delta$ we look for a configuration $X$. This achieves this by minimizing some loss function $\sigma_{MDS}(X^*;\Delta^*)$ where the $\Delta^*, X^*$ are functions of the $\Delta$ and $X$. The $X$ has properties with regards to its structural appearance, which we call _c-structuredness_ for configuration-structuredness. There are different types of c-structuredness people might be interested in (say, how clustered the result is, or that dimensions are orthogonal or if there is some submanifold that the data live on). We developed indices for these types of c-structuredness that capture that essence in the configuration.

We have as part of a STOPS model a proximity scaling loss function $\sigma_{MDS}(\cdot)$, a $\Delta$ and an $X$ and some transformation $f_{ij}(\delta_{ij};\theta)$ and $g_{ij}(d_{ij};\theta)$ that is parametrized (with $\theta$ either finite or infinite dimensional, e.g., a transformation applied to all observations like a power transformation or even an individual transformation per object). These transformations achieve a sort of push towards more structure, so different values for $\theta$ will in general lead to different c-structuredness.

We further have $K$ different indices $I_k(X)$ that measure different types of c-structuredness. We can then define \emph{STOPS} as methods that are of the form (additive STOPS)
\begin{equation}
  \text{aSTOPS}(X, \theta, v_0, \dots, v_k; \Delta) = v_0 \cdot \sigma_{MDS}(X^*(\theta)) + \sum^K_{k=1} v_k I_k(X(\theta))
\end{equation}
 or (multiplicative STOPS)
\begin{equation}
  \text{mSTOPS}(X, \theta, v_0, \dots, v_k; \Delta) = \sigma_{MDS}(X^*(\theta))^{v_0} \cdot \prod^K_{k=1} I_k(X(\theta))^{v_k} 
\end{equation}
(which can be expressed as aSTOPS by logarithms). Here the $v_0,...,v_k$ are weights that determine how the individual parts (mds loss and c-structuredness indices) are aggregated. 

<!-- [//]: Note that in this formulation the aggregation is a sum/product, so the weights must be negative if the mds loss should be penalized and a higher index stands for more structure. Alternatively, the `stressweight` can be negative and the `strucweight` positive. 
--!>


The job is then to find 
\begin{equation}  
 \arg\min_{\vartheta}\ \text{aSTOPS}(X, \theta, v_0, \dots, v_k; \Delta)\ \text{or} \ \arg\min_{\vartheta}\ \text{mSTOPS}(X, \theta, v_0, \dots, v_k; \Delta)
\end{equation} 
where $\vartheta \subseteq \{X,\theta, v_0, \dots, v_k\}$.  Typically $\vartheta$ will be a subset of all possible parameters here (e.g., the weights might be given _a priori_). Currently, the transformations that can be used in `stops` are limited to power transformations. 

Minimizing stoploss can be difficult. In `stops` we use a nested algorithm combining optimization that internally first solves for $X$ given $\theta$, $\arg\min_X \sigma_{MDS}\left(X,\theta\right)$, and then optimize over $\theta$ with a metaheuristic. Implemented are a simulated annealing (`optimmethod="SANN"`) or particle swarm optimization (`optimmethod="pso"`) and a variant of the Luus-Jaakola (`optimmethod="ALJ"`) procedure \citep{luus1973optimization}. We suggest to use the latter. A Bayesian optimization approach is currently under way.

Currently the following c-structuredness types are supported:

* **c-clusteredness** (`cclusteredness`): A clustered appearance of the configuration ($I_k$ is the normed OPTICS cordillera (COPS; Rusch et al. 2015a); 0 means no c-clusteredness, 1 means perfect c-clusteredness)
* **c-linearity** (`clinearity`): Projections lie close to a linear subspace of the configuration ($I_k$ is maximal multiple correlation; 0 means orthogonal, 1 means perfectly linear) 
* **c-manifoldness** (`cmanifoldness`): Projections lie on a sub manifold of the configuration ($I_k$ is maximal correlation (Sarmanov, 1958); only available for two dimensions; 1 means perfectly smooth function)
* **c-dependence** (`cdependence`): Random vectors of projections onto the axes are stochastically dependent ($I_k$ is distance correlation (Szekely et al., 2007); only available for two dimensions; 0 means they are stochastically independent)
* **c-association** (`cassociation`): Pairwise nonlinear association between dimensions ($I_k$ is the pairwise maximal maximum information coefficient (Reshef et al. 2011), 1 means perfect functional association)  
* **c-nonmonotonicity** (`cnonmonotonicity`): Deviation from monotonicity ($I_k$ is the pairwise maximal maximum assymmetry score (Reshef et al. 2011), the higher the less monotone)  
* **c-functionality** (`cfunctionality`): Pairwise functional, smooth, noise-free relationship between dimensions ($I_k$ is the mean pairwise maximum edge value (Reshef et al. 2011), 1 means perfect functional association)
* **c-complexity** (`ccomplexity`): Measures the degree of complexity of the functional relationship between any two dimensions ($I_k$ is the pairwise maximal minimum cell number (Reshef et al. 2011), the higher the more complex) 
* **c-faithfulness** (`cfaithfulness`): How accurate is the neighbourhood of $\Delta$ preserved in $D$ ($I_k$ is the adjusted Md index of Chen \& Buja, 2013; note that this index deviates from the others by being a function of both $X^*$ and $\Delta^*$ rather than $X^*$ alone)
* c-randomness: How close to a random pattern (under some model) is the configuration ($I_k$ is not clear yet; not yet implemented)

<!--[//]: c-evenness: How even and/or spread out are observations ($I_k$ is a function of the Ripley's K and L functions; not yet implemented) * c-randomness: How close to a random pattern (under some model) is the configuration ($I_k$ is not clear yet; not yet implemented) * c-separability: How well are classes (linear and nonlinear) separable. Perhaps only useful if there are labels. ($I_k$ is a separation index; not yet implemented) * c-sparsity: How sparse is the representation (percentage of vectors with 0 or percentage of weights that are zero; not yet implemented) * c-optimality: Finding the global minimum of stress by adding a penalty when the optimum is local ($I_k$ is Jan's idea; not yet implemented)
--!>

If we have a single $I(X)=OC(X)$, the OPTICS cordillera (Rusch et al. 2015a), and the transformations applied are power transformations and the weights for the $I(X)$ is negative we essentially have COPS (see below).    

For the MDS loss (argument `loss` in functions `stops` and `cops`), the functions currently support all losses derived from _powerstress_ and _powerstrain_ and can in principle be fitted with `powerStressMin` alone. However, for many models offer dedicated functions that either use workhorses that are more optimized for the problem at hand and/or restrict the parameter space for the distance/proximity transformations and thus can be faster. They are: 

* `stress`, `smacofSym`: Kruskall's stress; Workhorse: `smacofSym`, Optimization over $\lambda$
* `smacofSphere`: Kruskall's stress for projection onto a sphere; Workhorse `smacofSphere`, Optimizes over $\lambda$
* `strain`, `powerstrain`: Classical scaling; Workhorse: `cmdscale`, Optimization over $\lambda$
* `sammon`, `sammon2`: Sammon scaling; Workhorse: `sammon` or `smacofSym`, Optimization over $\lambda$
* `elastic`: Elastic scaling; Workhorse: `smacofSym`, Optimization over $\lambda$
* `sstress`: S-stress; Workhorse: `powerStressMin`, Optimization over $\lambda$ 
* `rstress`: S-stress; Workhorse: `powerStressMin`, Optimization over $\kappa$
* `powermds`: MDS with powers; Workhorse: `powerStressMin`, Optimization over $\kappa$, $\lambda$
* `powersammon`: Sammon scaling with powers; Workhorse: `powerStressMin`, Optimization over $\kappa$, $\lambda$
* `powerelastic`: Elastic scaling with powers; Workhorse: `powerStressMin`, Optimization over $\kappa$, $\lambda$
* `powerstress`: Power stress model; Workhorse: `powerStressMin`, Optimization over $\kappa$, $\lambda$, $\nu$
 
#### Usage 

The syntax for fitting a `stops` model is rather straightforward. One has to supply the arguments `dis` which is a dissimilarity matrix and `structures` a character vector listing the c-structuredness type that should be used to augment the PS loss (see above for the types of structures and losses). The parameters for the structuredness indices should be given with `strucpars`, a list whose elements are lists corresponding to each structuredness index and listing the parameters (if the default should be used the list element should be set to `NULL`). The PS loss can be chosen with the argument `loss`. The type of aggregation for the multi-objective optimization is specified in `type` and can be one of `additive` or `multiplicative`. One can pass additional parameters to the fitting workhorses with `...`.

`stops(dis, structures = c("cclusteredness","clinearity"), loss="stress", ...)`

One then has all the S3 methods of `smacofP` at one's disposal.

For example, let us fit an mSTOPS model that looks for a transformation of the $\delta_{ij}$ so that a) the result has maximal c-clusteredness (which is 1 in the best case, so we set a negative weight for this structure) b) the projection onto the principal axes are nearly orthogonal (c-clinearity close to 0, so we set a positive weight for this structure) c) but the projections onto the principal axes should be stochastically dependent (negative weight on c-dependence) and d) the fit of the MDS is also factored in (so we set positive weight on the MDS loss). Since we use mSTOPS and a negative weight for c-linearity and c-dependence, a c-linearity/c-dependence close to 0 will overall dominate the stoploss function with the other two criteria being more of an afterthought - in aSTOPS that would be different. 

[!!]: This is generally the approach to be chosen: We _minimize_ the stoploss, so a c-structuredness index that should be (numerically) large needs a _negative weight_ and a c-structuredness index that should be (numerically) small needs a _positive weight_.  

We first set up the parameters for the structuredness indices. For the OPTICS cordillera we use a $d_{max}$ of 1, epsilon=10 and minpts=2, for c-linearity we have no parameters (so using `NULL` will work) and for the c-dependence we have a single parameter, `index`, which we set to 2.   

```{r}
strucpars<-list(list(epsilon=10,minpts=2,rang=c(0,1.3)), #cordillera
                NULL,     # c-linearity (has no parameters)
                list(index=2) #c-dependence
                ) 

```

```{r}
ressm<-stops(kinshipdelta,loss="stress",stressweight=1,structures=c("cclusteredness","clinearity","cdependence"),strucweight=c(-0.33,0.33,-0.33),verbose=0,strucpars=strucpars,type="multiplicative")
ressm
```

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(ressm)
```

Let us compare this with the corresponding aSTOPS

```{r}
ressa<-stops(kinshipdelta,loss="stress",stressweight=1,structures=c("cclusteredness","clinearity","cdependence"),strucweight=c(-0.33,0.33,-0.33),verbose=0,strucpars=strucpars,type="additive")
ressa
```

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(ressa)
```

We see that the c-clusteredness is higher as compared to the mSTOPS result - we have a number of distinct object clusters (with at least minpts=2) and they are more spread out and distributed more evenly. The dimensions on the other hand are now farther from being orthogonal but the stochastic dependence is higher (which is a non-linear one obviously). 

When choosing a c-structuredness index, one needs to be clear what structure one might be interested in and how it interacts with the PS loss chosen. Consider the following example: We fit a `powermds` model to the kinship data and want to maximize c-association (i.e., any non-linear relationship) and c-manifoldness but minimize the c-linearity. In other words we try to find a power transformation of $\Delta$ and $D$ so that the objects are positioned in the configuration in such a way that the projection onto the principal axes are as close as possible to being related by a smooth but non-linear function. 

```{r,eval=FALSE}
resa<-stops(kinshipdelta,structures=c("cassociation","cmanifoldness","clinearity"),loss="powermds",verbose=0,strucpars=list(NULL,NULL,NULL),type="additive",strucweight=c(-0.5,-0.5,0.5))
```

```{r,echo=FALSE,eval=FALSE}
set.seed(210485)
ressax<-stops(kinshipdelta,theta=c(1.88,1.00,1),loss="powermds",stressweight=0,structures=c("cclusteredness"),strucweight=c(-1),verbose=3,strucpars=list(list(epsilon=10,minpts=2,rang=c(0,1.3))),type="additive")
ressax
set.seed(210485)
ressax2<-cops(kinshipdelta,theta=c(1.88,1.00,1),loss="powermds",stressweight=0,cordweight=c(1),verbose=3,epsilon=10,minpts=2,rang=c(0,1.3))
ressax2
ressax3<-cops(kinshipdelta,loss="powermds",stressweight=0,cordweight=c(1),verbose=3,epsilon=10,minpts=2,rang=NULL)
ressax3
resa1<-stops(kinshipdelta,theta=c(2,1.00,1),structures=c("cassociation"),loss="powermds",verbose=3,strucpars=list(list(NULL)),type="additive",strucweight=-1,stressweight=0,acc=1e-16)
resa2<-stops(kinshipdelta,theta=c(3.066,1.001,1.388),structures=c("cclusteredness"),loss="powermds",verbose=3,strucpars=list(list(epsilon=10,rang=c(0,1.3),minpts=2)),type="additive",strucweight=-1,stressweight=0)
resa5<-stops(kinshipdelta,theta=c(3.066,1.001,1.388),structures=c("cclusteredness"),loss="powermds",verbose=3,strucpars=list(list(epsilon=10,rang=c(0,1.3),minpts=5)),type="additive",strucweight=-1,stressweight=0)
```

```{r,echo=FALSE,eval=TRUE}
resa<-stops(kinshipdelta,theta=c(2.9429394,1.67850653,1.57140404), structures=c("cassociation","cmanifoldness","clinearity"),loss="powermds",verbose=0,strucpars=list(NULL,NULL,NULL),type="additive",strucweight=c(-0.5,-0.5,0.5),itmax=1)
```

```{r}
resa
```

We see in this model (`resa`) that indeed the c-association is 1, which says we have a near perfect non-linear relationship. How does this relationship look like? 

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(resa)
```

It is a parabolic shape, so the projections are so that the points on D2 are a near parabolic function of the D1 (projecting onto some structure resembling a conic section is often the case for r-stress which is essentially what we got here - setting a negative weight on c-assocation can combat that if that is an artefact). What we can also see is that there are three clear clusters, so c-clusteredness should be high. But when looking at the OPTICS cordillera here, we find that the OPTICS cordillera is lower than for the result from above with using `stress` and lambda=2.66 (the `ressa` model).

```{r}
c1<-cordillera(resa$fit$conf,minpts=2,epsilon=10,rang=c(0,1.3))
c2<-cordillera(ressa$fit$conf,minpts=2,epsilon=10,rang=c(0,1.3))
c1
c2
```    

This discrepancy comes from the definition of c-clusteredness (Rusch et al, 2015a) where more clusters, more spread-out clusters, more evenly distributed clusters and denser clusters all increase c-clusteredness. In the example with maximizing c-association we have two very dense clusters of 5 points and 1 relatively non-dense cluster of five other points. In the model maximizing c-clusteredness (and others) we get 6 relatively moderate dense clusters with 2 or 3 points each, which is also the minimum number of points we wanted to be grouped together. Most importantly, they are projected onto a much larger range of the target space as the $X$ obtained from the `stress` loss is different than the one obtained from the `powermds` loss, so the $d_{max}$ is very different. Since we use the normed OPTICS cordillera there, we look at c-clusteredness relative to the most clustered appearance with two points per cluster. Thus, the second result has more c-clusteredness. If we would define a cluster as having at most 5 points then c-clusteredness of the result with high c-association also has a large c-clusteredness because then the clusters found match the definition of high c-clusteredness. 

```{r}
c3<-cordillera(resa$fit$conf,minpts=5,epsilon=10,rang=c(0,1.3))
c3
```      
Note that it may just as well be possible to have a high c-association and no c-clusteredness at all (e.g., points lying equidistant on a smooth non-linear curve). Note also that the models are not necessarily comparable due to different stress functions - the transformation in `powermds` that is optimal with respect to c-clusteredness would be different. 

Indeed one can optimize for c-clusteredness alone and using it as a "goodness-of-clusteredness" index (i.e., the $d_{max}$ is not constant over configurations but varies conditional on the configuration) then we get a projection with c-clusteredness of 0.67.  
 
```{r,echo=FALSE,eval=FALSE}
resa2<-stops(kinshipdelta,structures=c("cclusteredness"),loss="powermds",verbose=0,strucpars=list(epsilon=10,rang=c(0,1.3),minpts=2),type="additive",strucweight=-1,stressweight=0)
cordillera(resa2$fit$conf,epsilon=5.52,rang=c(0,2.12225),q=10)
#huge cordillera - becausee this way q is set to 10 as we only say "eps" - epsilon is no problem?
resa2<-stops(kinshipdelta,structures=c("cclusteredness"),loss="powermds",verbose=0,strucpars=list(list(eps=10,rang=NULL,minpts=2)),type="additive",strucweight=-1,stressweight=0)
```

```{r,eval=FALSE}
resa2<-stops(kinshipdelta,structures=c("cclusteredness"),loss="powermds",verbose=0,strucpars=list(list(epsilon=10,rang=NULL,minpts=2)),type="additive",strucweight=-1,stressweight=0)
```

For convenience it is also possible to use the `stops` function for finding the loss-optimal transformation in the the non-augmented models specified in `loss`, by setting the `strucweight`, the weight of any c-structuredness, to 0. Then the function optimizes the MDS loss function only. 

```{r,eval=FALSE}
ressa<-stops(kinshipdelta,structure=c("clinearity"),strucweight=0,loss="stress",verbose=0)
```


## Other Functions
The package also provides functions that are used by the `cops` and `stops` and `powerStressMin` functions but may be of interest to a end user beyond that.

### OPTICS and OPTICS cordillera
For calculating a COPS solution, we need the OPTICS algorithm and the OPTICS cordillera. In the package we also provide a rudimentary interface to the OPTICS impementation in ELKI. 

```{r}     
data(iris)
res<-optics(iris[,1:4],minpts=2,epsilon=1000)
print(res)
summary(res)
```

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(res,withlabels=TRUE)
```

and a function for calculating and displaying the OPTICS cordillera.      

```{r,fig.show='hold',fig.width=8,fig.height=8}
cres<-cordillera(iris[,1:4],minpts=2,epsilon=1000,scale=FALSE)
cres
summary(cres)
```

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(cres)
```

### Optimization
Since the inner optimization problem in STOPS models is hard and takes long, Rusch et al. (2015a) developed a metaheuristic for the outer optimization problem that needs typically less calls to the inner minimization than `pso` or `SANN`, albeit without the guarantees of convergence to a global minimum for non-smooth functions. It is an adaptation of the Luus-Jaakola random search (Luus & Jaakola 1973). It can used with the function `ljoptim` which modeled its output after `optim`. It needs as arguments `x` a starting value, `fun` a function to optimize, a `lower` and `upper` box constraint for the search region. By using the argument `adaptive=TRUE` or `FALSE` one can switch between our adaptive version and the original LJ algorithm. Accuracy of the optimization can be controlled with the `maxit` (maximum number of iterations), `accd` (terminates after the length of the search space is below this number ) and `acc` arguments (terminates if difference of two subsequent function values are below this value).  

We optimize a "Wild Function" with the non-adaptive LJ version (and numerical accuracies of at least `1e-16` for `accd` and `acc`).
 
```{r}
set.seed(210485)
fwild <- function (x) 10*sin(0.3*x)*sin(1.3*x^2) + 0.00001*x^4 + 0.2*x+80
res2<-ljoptim(50, fwild,lower=-50,upper=50,adaptive=FALSE,accd=1e-16,acc=1e-16)
res2
```

```{r,fig.show='hold',fig.width=8,fig.height=8}
plot(fwild, -50, 50, n = 1000, main = "ljoptim() minimising 'wild function'")
points(res2$par,res2$value,col="red",pch=19)
```    

### Procrustes Adjustment
We also provide a procrustes adjustment to make two configurations visually comparable. The function is `conf_adjust` and takes two configurations `conf1` the reference configuration and `conf2` another configuration. It returns the adjusted versions

```{r,eval=FALSE}
conf_adjust(conf1,conf2) 
```


##References

* Borg I, Groenen PJ (2005). Modern multidimensional scaling:  Theory and applications.  2nd edition. Springer, New York

* Buja A, Swayne DF, Littman ML, Dean N, Hofmann H, Chen L (2008). Data visualization with multidimensional scaling. Journal of Computational and Graphical Statistics, 17 (2), 444-472.

* Chen L, Buja A (2013). Stress functions for nonlinear dimension reduction, proximity analysis, and graph drawing. Journal of Machine Learning Research, 14, 1145-1173.

* de Leeuw J (2014). Minimizing r-stress using nested majorization. Technical Report, UCLA, Statistics Preprint Series.

* de Leeuw J, Mair P (2009). Multidimensional Scaling Using Majorization:  SMACOF in R. Journal of Statistical Software, 31 (3), 1-30. 

* Kruskal JB (1964).  Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29 (1), 1-27.

* Luus R, Jaakola T (1973).  \Optimization by direct search and systematic reduction of the size of search region. American Institute of Chemical Engineers Journal (AIChE), 19 (4), 760-766.

* McGee VE (1966). The multidimensional analysis of 'elastic' distances. British Journal of Mathematical and Statistical Psychology, 19 (2), 181-196.

* Reshef D, Reshef Y, Finucane H, Grossman S, McVean G, Turnbaugh P, Lander E, Mitzenmacher M, Sabeti P (2011). Detecting novel associations in large datasets. Science, 334, 6062.

* Rosenberg, S. & Kim, M. P. (1975). The method of sorting as a data gathering procedure in multivariate research. Multivariate Behavioral Research, 10, 489-502.

* Rusch, T., Mair, P. and Hornik, K. (2015a) COPS: Cluster Optimized Proximity Scaling. Discussion Paper Series / Center for Empirical Research Methods, 2015/1. WU Vienna University of Economics and Business, Vienna.

* Rusch,  T.,  Mair,  P. and Hornik, K. (2015b). Structuredness  Indices  and  Augmented  Nonlinear Dimension Reduction. In preparation.

* Sammon JW (1969). A nonlinear mapping for data structure analysis. IEEE Transactions on Computers, 18 (5), 401-409

* Sarmanov OV (1958) The maximum correlation coefficient (symmetric case). Dokl. Akad. Nauk SSSR, 120 : 4 (1958), 715 - 718.

* Székely, G. J. Rizzo, M. L. and Bakirov, N. K. (2007). Measuring and testing independence by correlation of distances, The Annals of Statistics, 35:6, 2769–2794.

* Takane Y, Young F, de Leeuw J (1977). Nonmetric individual differences multidimensional scaling: an alternating least squares method with optimal scaling features. Psychometrika, 42 (1), 7-67.

* Torgerson WS (1958). Theory and methods of scaling. Wiley.


* Venables WN, Ripley BD (2002). Modern Applied Statistics with S. Fourth edition. Springer, New York. 




